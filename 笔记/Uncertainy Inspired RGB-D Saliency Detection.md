# Uncertainy Inspired RGB-D Saliency Detection

## Introduction：

​	    传统匹配R他显著图是有问题的。GT标签化过程不是一个确定性个过程。对于每个人来说他所看到的一张图的显著图都是有差异的。所以构造了一个模型来实现RGB-D的概率显著检测和一个潜在的变量来捕捉人类标注不确定性的影响。

​		**主要有两个模型组成**

​	1.生成器模型：匹配随机显著性检测和RGB-D输入数据以及潜在的变量。

​	2.一个推测模型：一直更新潜在变量。

### 推出潜在变量：

1.CVAE和附加编码器近似其后验分布

2.ABP（基于MCMC采样）从真实后验分布直接采样潜在变量

​	使用**KL退火**（**KL-annealing**）来减轻后验坍塌。

​	减少UC-NET成本函数来减少训练和测试编码潜在变量的差异。

**建议使用ABP来构建模型**

​	如果模型更注重优化重建质量，则潜在空间可能无法学习有意义的表示。如果模型更注重优化质量，则潜在空间可能无法学习有意义的表示。如果模型更注重减少后验于显眼分部之间的差异，则模型可能会牺牲重建质量。

### 改模型的主要贡献：

1.  引入Z来表示人类的不确定性。
2. 两种方案推断潜在变量。

### 模型：

​	训练期间，从“生成器”中学习显著性，并且更新潜在变量（用推进器）测试时，从变量“先验“分布中采样，已获得随机性显著预测

#### **生成器**

![Snipaste_2022-01-24_11-13-22](C:\Users\asus\Desktop\学习\实验室CV文章\笔记\截图\Snipaste_2022-01-24_11-13-22.png)

PAM：收集空间信息

CAM：收集通道信息

#### **推导器**

- 潜在变量Z，由$P_{\theta}(z|x)$的高斯分布抽取

- 输出变量Y，由$P_{w}(Y|X_z)$ 产生

- 条件生成器模型：

- 1. 条件变量x

  2. 潜在变量z

  3. 输出变量y

- 损失函数：$L_{CVAE} = E_{z}~Q_{\phi}(z|x,y)[-logP_{w}(y|x,z)] + \lambda_{kl}*D_{kl}(Q_{\phi}(z|x,y)||P_{\theta}(z|x))$。

- $\lambda_{kl}$用来缓解后验坍塌问题，从0到1的循环，$\lambda=ep/N_{ep} $ ，其中ep是当前的epoch，$N_{ep}$则是最大的epoch数（及一共有几轮训练）
- 对于推导器，在CVAE训练阶段，z这个潜在变量的损失函数是由他的后验分布承担的，就为 $z=Q_{\phi}(Y|x,z)$ 。但是在测试阶段，用的是Z的先验概率。
- GSNN(Gaussian Stochastic Neural Network)，用来缓解在编码阶段在潜在变量z身上存在的测试与训练时的差异（另一种方案是KL退火），就是令后验网络与先验网络相同，然后就可以直接从训练或者测试的鲜艳网络中直接提取z，公式为 $L_{GSNN}=E_{z\sim P_{\theta}(z|x)}[-logP_{w}(y|x,z)]$ 

#### **合并统一模型**

![](C:\Users\asus\Desktop\学习\实验室CV文章\笔记\截图\Snipaste_2022-01-28_19-40-35.png)

- 这个模型实际上是由以上两个函数结合而来，就是CAVE和GSNN，他的对应联合函数是 $L_{hybrid}=\alpha L_{CAVE}+(1-\alpha)L_{GSNN}$ 
- 可以同时将X和Y作为输入的一部分传入网络，$P_{\theta}(z|x)$ 定义为先验网络，可以将RGB-D中的输入数据X映射到一个更低维度的潜在特征空间，$\theta$ 就是这个网络的参数集
- 通过GT显著图可以与Y进行联系，定义了 $Q_{\phi}(z|x,y)$ 为后验网络，$\phi$ 是他的参数集
- 通过使用5个卷积层和2个全连接层来将输入的 RGB-D 图像 X（或对于 后验网络的 X 和 Y 的串联）映射到潜在空间的统计信息：先验网络的 $(\mu_{prior},\sigma_{prior})$ ，和后验网络的 $(\mu_{post},\sigma_{post})$  

![Snipaste_2022-01-28_20-03-04](C:\Users\asus\Desktop\学习\实验室CV文章\笔记\截图\Snipaste_2022-01-28_20-03-04.png)

​		然后可以通过重新参数化技巧获得相应的潜在向量 $z=\mu+\sigma*\theta$ 且 $\theta$ ~ $N(0,1)$ 

- $\alpha$ 是用于平衡GSNN和CAVE的

### 通过ABP推断z

- ABP被用来学习生成器网络模型
- 

## 注释：

- ANN为人工神经网络
- ABP为交替反向传播网络

